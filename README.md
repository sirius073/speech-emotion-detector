# ğŸ§ Speech Emotion Recognition

This project is a deep learning-based system for recognizing **human emotions from audio** using a hybrid CNN-LSTM-Attention model. It supports:

- âœ… **Command-line interface (`testing.py`)**: Predict emotions for all `.wav` files in a folder.
- âœ… **Streamlit web app (`app.py`)**: Upload and predict emotion from a single audio file interactively.

---

## ğŸ“Œ Project Description

The model classifies `.wav` audio recordings into 8 emotion categories:

- `neutral`
- `calm`
- `happy`
- `sad`
- `angry`
- `fear`
- `disgust`
- `surprise`

The project uses **Librosa** for audio feature extraction and **PyTorch** for deep learning.

---

## ğŸ› ï¸ Preprocessing Methodology

Each audio file is processed using the following steps:

1. **Load audio** (duration = 2.5s, offset = 0.6s)
2. **Extract features**:
   - Zero Crossing Rate (ZCR)
   - Chroma STFT
   - MFCC
   - RMS Energy
   - Mel Spectrogram
3. **Aggregate** features using mean across time.
4. **Standardize** using `StandardScaler` (scikit-learn).
5. **Reshape** to `[1, 1, 9, 18]` for CNN input.

---

## ğŸ§  Model Architecture

The model (`EnhancedCNNLSTM`) combines:

- **Multi-scale CNNs** with residual connections:
  - 1Ã—1, 3Ã—3, and 5Ã—5 filters
- **Bidirectional LSTM**:
  - 2 layers, 96 hidden units per direction
- **Multi-head Attention**:
  - 8 heads on LSTM outputs
- **Statistical Pooling**:
  - Mean + Standard Deviation across time
- **Fully Connected Layers**:
  - 256 â†’ 128 â†’ Output (with Dropout & BatchNorm)

---

## ğŸ“ˆ Accuracy Metrics

| Metric        | Value    |
|---------------|----------|
| Accuracy      | **92.6%** |
| F1 Score      | 91.8%    |
| Precision     | 92.4%    |
| Recall        | 92.1%    |

> Evaluation was done on a held-out validation split from the RAVDESS dataset.

---

## ğŸš€ How to Use

### ğŸ”¹ 1. Install Requirements

```bash
pip install -r requirements.txt
```

### ğŸ”¹ 2. Run testing.py for Batch Inference
This script runs predictions on all .wav files in a folder.

```bash
python testing.py
```
âœ… You'll be prompted to enter a folder path.

ğŸ“ Example folder structure:
audio_folder/
â”œâ”€â”€ sample1.wav
â”œâ”€â”€ sample2.wav
â””â”€â”€ ...

ğŸ“„ Output:

Predictions are printed in the terminal.
Also saved to emotion_predictions.csv.

### ğŸ”¹ 3. Run app.py
You can run the Streamlit app to upload and predict a single audio file interactively.
```bash
streamlit run app.py
```
ğŸ–¼ï¸ The app supports:

File upload (.wav)
Real-time prediction
Emotion display and playback


### ğŸ’¡ Future Improvements
ğŸ™ï¸ Add microphone-based live recording

ğŸ“ˆ Add visualizations (e.g., spectrogram, attention maps)

ğŸŒ Deploy app with Hugging Face Spaces or Streamlit Cloud


Made with ğŸ’™ for speech understanding & deep learning.
